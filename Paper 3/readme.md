# A human study of comprehension and code summarization

###  Authors:
Sean Stapleton, Yashmeet Gambhir, Westley Weimer, Kevin Leach and Yu Huang.

>Conference:
Tue 14 Jul 2020 02:00 - 02:15 at ICPC - Session 4: Summalization Chair(s): Venera Arnaoudova.

[Paper lnk](https://dijkstra.eecs.umich.edu/kleach/icpc2020-code-summarization.pdf "click here to view paper")

>Media link: https://youtu.be/V9h5mPHf4Hg

# **Introduction and motivation**:

This passage illustrates the efforts of Sean Stapleton, Yashmeet Gambhir, Westley Weimer, Kevin Leach and Yu Huang from the university of Michigan and Alexander LeClair and Zachary Eberhart from the university of Notre Dame in bringing about the development of machine-generated coding. They studied how different summarization techniques affect human comprehension and productivity and assessed these using metrics such as BLEU and ROUG. However, these did not provide the predicted results. Therefore, they conducted experiments with two groups of university students and gave them the task of coding using java and answering comprehension questions that followed. The first group of students were given human-written text while the other group was given machine generated text. The participants coped well enough with human written text than machine generated text, although no evidence suggested the quality difference between the two. Furthermore, the correlation of BLEU and ROUGE with machine generated coding was left unproven. 
Well documented source code enables a developer to enhance creativity while investigating existing codes for the first time or while reviewing maintenance of large codebases. Source code comments play an important role in helping developers answer comprehensive questions regarding source codes and in the maintenance of various programming tasks. However, most comments fail to make complete sense in practice. In a study, xia et al, insufficiently- commented codes raised difficulties in program comprehension. In order to overcome these difficulties, researches have come up with ways to automatically generate comments for source code that use heuristics and natural language templates. Furthermore, recently, new techniques have come to existence that make use of deep learning and publicly available repositories. These have turned out to be a great success as they greatly resemble human-written comments. Although, these still have room for improvement as they generally do not express what the function does or its intended purpose. Its main root is statistical evaluation metrics such as BLEU and ROUGE which compare generated text to the reference one without considering the meaning. However, since it shows no correlation between such metrics and automatic code summarization, hence, the code summarization is often accompanied by human study done by programmers.
Although it allows different techniques to be compared yet they do not show the reasonable extent to which automatically generated comments help in program comprehension. Therefore, extrinsic quality of machine generated summaries is preferred which allow programmers to engage in code comprehension tasks aided by machine generated comments.

# **Methodology**:

The researchers performed various tasks and came up with several models to figure out the best possible outcome for automatic summarization. This section describes the methodologies for measuring the impact of code summaries. An IRB-approved human study came forward which involved 45 computer science students and industrial developers with less than 5 years’ experience who were required to complete two tasks in an online survey. Firstly, they were asked to answer comprehension questions based on java and associated summaries. Secondly, they were required to complete partially completed java classes that included summaries and a method in the class with respect to held-out test suite. They measured performance differences by controlling whether documentation was human-written or machine-generated among sub-populations of participants who received one type of summary compared to another. Participant data was anonymized, but they could optionally leave contact information to obtain a $20 USD cash reward for their participation. To filter the 2.1 million snippets in this dataset, we first limited the population of snippets to only those that had 6 or more immediate children in the body of the code’s Abstract Syntax Tree (AST). We found that this heuristic significantly increased the quality of the code over more rudimentary approaches such as filtering by line length. We chose our final set of 50 methods by uniformly sampling from this sub-population. These methods contained between 1 and 5 control flow statements, between 0 and 17 method invocations, and between 0 and 3 parameters. For each participant and method, we randomly presented a human-written or machine-generated summary for that method. We visited the AST of each method to obtain a list of invoked methods for which we presented summaries as well. The participant was shown one entire method at a time, its summary, and a list of methods invoked (statically) with their corresponding summaries. Participants were shown a single question at a time. Thus, the results were used to develop a model of the developer’s productivity. 
Participants also completed a high-level code writing task which involved implementing a new class method based upon summaries of other methods in a class. For each task, participants were shown a semi-complete Java data structure class taken from a entry-level course such as a Binary Tree, a Singly-Linked List, or an Adjacency Matrix. These data structures were selected because they did not require domain-specific expertise and as such a data structure was randomly chosen and assigned either human-written or machine-generated summaries We removed one method from the class at random and tasked participants with implementing the held-out method given a method description as well as the other methods and summaries present in the class.
We collected two types of data particularly comprehension, where participatants read code and summaries and answered comprehension questions and implementation which involved participants to complete a java class and write a missing method based on a held-out test suit. For a comprehension, participants were given a free space to write an open answer to each comprehension question, and a rubric for each snippet-question pair to provide a robust quantitative assessment of participant data. The answers were ranked on a 1 to 5 scale, where 1 indicated a low quality response and 5 indicated a high quality response We refer to this scale as rater-assessed Correctness. Four raters graded all responses with respect to these snippet-question rubrics. In addition, Participants wrote and submitted code online that automatically evaluated their submission against a held-out test suite. the fraction of passed test cases as a proxy were used for correctness in conjunction with the number of submissions required to attain their highest score. For the comprehension questions, only considered data was marked as Complete and Relevant by all graders. Additionally, because a web survey was used, any answers that took longer than 30 minutes to complete were excluded.

# **Results**:

First, find that human-written summaries were found to help developers comprehend code significantly better than machine-generated summaries. Second, developer perception of summary quality, whether human-written or machine-generated, did not significantly correlate with developer comprehension. Finally,it was evident that BLEU and ROUGE scores were significantly uncorrelated (i.e., ρ = 0.151 with p = 0.0004 for ROUGE and ρ = 0.140 with p = 0.0008 for BLEU) with developer comprehension—developers do not benefit from summaries with higher-valued BLEU or ROUGE scores. This indicated a need for new metrics for measuring automatic summarization techniques.


